<!DOCTYPE html>
<html>
  <head>
    <title>View-limited Page</title>
  </head>
  <body>
   <div id="hack" hidden><pre>
    2) i)PROGRAM:
    hadoop dfs –mkdir  /directory_name
    hadoop dfs -ls /
    hadoop dfs –put /localfile /hdfsdirectory
    hadoop dfs -get /HDFSFile /localpath
    hadoop dfs -cat /folder/filename
    hadoop dfs –rm /path
    hadoop dfs -rmdir -r /Foldername
    
    3) A) PROGRAM:
    import java.io.IOException;
    import java.util.StringTokenizer;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.Reducer;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    public class WordCount {
      public static class TokenizerMapper
           extends Mapper<Object, Text, Text, IntWritable>{
    
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();
    
        public void map(Object key, Text value, Context context
                        ) throws IOException, InterruptedException {
          StringTokenizer itr = new StringTokenizer(value.toString());
          while (itr.hasMoreTokens()) {
            word.set(itr.nextToken());
            context.write(word, one);      }    }  }
      public static class IntSumReducer
           extends Reducer<Text,IntWritable,Text,IntWritable> {
        private IntWritable result = new IntWritable();
        public void reduce(Text key, Iterable<IntWritable> values,
                           Context context
                           ) throws IOException, InterruptedException {
          int sum = 0;
          for (IntWritable val : values) {
            sum += val.get();
          }
          result.set(sum);
          context.write(key, result);
        }
      }
      public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
      job.setMapperClass(TokenizerMapper.class);
       job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
      }
    }
    hdfs dfs -mkdir programs
    hdfs dfs -put input.txt programs/
    hadoop jar wordcount.Jar wordcount programs/input.txt programs/output
    hdfs dfs -cat programs -word/output/*
    
    
    3)B) Wordsearch
    
    import java.io.IOException;
    import java.util.StringTokenizer;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.Reducer;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    public class WordSearch {
        public static class TokenizerMapper
                extends Mapper<Object, Text, Text, IntWritable>{
            private final static IntWritable one = new IntWritable(1);
            private Text word = new Text();
    
            public void map(Object key, Text value, Context context
            ) throws IOException, InterruptedException {
                Configuration conf = context.getConfiguration();
                String search = conf.get("search");
    
                StringTokenizer itr = new StringTokenizer(value.toString());
                while (itr.hasMoreTokens()) {
                    String token = itr.nextToken();
                    if(token.startsWith(search)) {
                        word.set(token);
                        context.write(word, one);
                    }       }     }    }
        public static class IntSumReducer
                extends Reducer<Text,IntWritable,Text,IntWritable> {
            private IntWritable result = new IntWritable();
    
            public void reduce(Text key, Iterable<IntWritable> values,
                               Context context
            ) throws IOException, InterruptedException {
                int sum = 0;
                for (IntWritable val : values) {
                    sum += val.get()  }
                result.set(sum);
                context.write(key, result); }}
        public static void main(String[] args) throws Exception {
            Configuration conf = new Configuration();
            conf.set("search", args[2]);
    
            Job job = Job.getInstance(conf, "wordsearch");
            job.setJarByClass(WordSearch.class);    job.setMapperClass(TokenizerMapper.class);
          job.setReducerClass(IntSumReducer.class);
            job.setOutputKeyClass(Text.class);      job.setOutputValueClass(IntWritable.class);
          FileInputFormat.addInputPath(job, new Path(args[0]));
            FileOutputFormat.setOutputPath(job, new Path(args[1]));
            System.exit(job.waitForCompletion(true) ? 0 : 1); }}
    hadoop jar wordsearch.jar grep programs/input.txt programs/output programs
    hdfs dfs -cat programs/output/*
    
    
    
    4) PROGRAM:
    
    import java.io.IOException;
    import java.util.HashSet;
    import java.util.Set;
    import java.util.StringTokenizer;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Counters;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.Reducer;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    public class StopWords {
     private static final String[] stopWords = { "a", "about", "above", "above",
       "across", "after", "afterwards", "again", "against", "all",
       "almost", "alone", "along", "already", "also", "although",
       "always", "am", "among", "amongst", "amoungst", "amount", "an",
       "and", "another", "any", "anyhow", "anyone", "anything", "anyway",
       "anywhere", "are", "around", "as", "at", "back", "be", "became",
       "because", "become", "becomes", "becoming", "been", "before",
       "beforehand", "behind", "being", "below", "beside", "besides",
       "between", "beyond", "bill", "both", "bottom", "but", "by", "call",
       "can", "cannot", "cant", "co", "con", "could", "couldnt", "cry",
       "de", "describe", "detail", "do", "done", "down", "due", "during",
       "each", "eg", "eight", "either", "eleven", "else", "elsewhere",
       "empty", "enough", "etc", "even", "ever", "every", "everyone",
       "everything", "everywhere", "except", "few", "fifteen", "fify",
       "fill", "find", "fire", "first", "five", "for", "former",
       "formerly", "forty", "found", "four", "from", "front", "full",
       "further", "get", "give", "go", "had", "has", "hasnt", "have",
       "he", "hence", "her", "here", "hereafter", "hereby", "herein",
       "hereupon", "hers", "herself", "him", "himself", "his", "how",
       "however", "hundred", "ie", "if", "in", "inc", "indeed",
       "interest", "into", "is", "it", "its", "itself", "keep", "last",
       "latter", "latterly", "least", "less", "ltd", "made", "many",
       "may", "me", "meanwhile", "might", "mill", "mine", "more",
       "moreover", "most", "mostly", "move", "much", "must", "my",
       "myself", "name", "namely", "neither", "never", "nevertheless",
       "next", "nine", "no", "nobody", "none", "noone", "nor", "not",
       "nothing", "now", "nowhere", "of", "off", "often", "on", "once",
       "one", "only", "onto", "or", "other", "others", "otherwise", "our",
       "ours", "ourselves", "out", "over", "own", "part", "per",
       "perhaps", "please", "put", "rather", "re", "same", "see", "seem",
       "seemed", "seeming", "seems", "serious", "several", "she",
       "should", "show", "side", "since", "sincere", "six", "sixty", "so",
       "some", "somehow", "someone", "something", "sometime", "sometimes",
       "somewhere", "still", "such", "system", "take", "ten", "than",
       "that", "the", "their", "them", "themselves", "then", "thence",
       "there", "thereafter", "thereby", "therefore", "therein",
       "thereupon", "these", "they", "thickv", "thin", "third", "this",
       "those", "though", "three", "through", "throughout", "thru",
       "thus", "to", "together", "too", "top", "toward", "towards",
       "twelve", "twenty", "two", "un", "under", "until", "up", "upon",
       "us", "very", "via", "was", "we", "well", "were", "what",
       "whatever", "when", "whence", "whenever", "where", "whereafter",
       "whereas", "whereby", "wherein", "whereupon", "wherever",
       "whether", "which", "while", "whither", "who", "whoever", "whole",
       "whom", "whose", "why", "will", "with", "within", "without",
       "would", "yet", "you", "your", "yours", "yourself", "yourselves" };
     static Set<String> stopWordsSet = new HashSet<>();
    static {
      for (String s : stopWords) {
       stopWordsSet.add(s);  } }
     public static enum COUNTERS {
      STOPWORDS;}
     public static class StopWordsMapper extends
       Mapper<Object, Text, Text, IntWritable> {
      public boolean isStopWord(String s) {
       return stopWordsSet.contains(s);}
      public void map(Object key, Text value, Context context)
        throws IOException, InterruptedException {
       String str = value.toString();
       StringTokenizer tokens = new StringTokenizer(str);  while (tokens.hasMoreElements()) {
        String word = tokens.nextToken();
      if (isStopWord(word)) {
    Context.getCounter(COUNTERS.STOPWORDS).increment(1); }}}}
     public static void main(String[] args) throws Exception {
      Configuration conf = new Configuration();
      Job job = Job.getInstance(conf, "Character count");
      job.setJarByClass(StopWords.class);
    job.setMapperClass(StopWordsMapper.class);
      job.setOutputKeyClass(Text.class);
      job.setOutputValueClass(IntWritable.class);
      FileInputFormat.addInputPath(job, new Path(args[0]));
      FileOutputFormat.setOutputPath(job, new Path(args[1]));
      job.waitForCompletion(true);
      org.apache.hadoop.mapreduce.Counters counters = job.getCounters();
      System.out.printf("Number of stop words are %d ",;counters.findCounter(COUNTERS.STOPWORDS).getValue());}}
    
    stop words:
    
    import sys
    import re
    
    # Define the list of stop words
    stop_words = set(['a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with'])
    
    # Define a regular expression to match words
    word_regex = re.compile(r"\b\w+\b")
    
    # Read each line from stdin
    for line in sys.stdin:
        # Split the line into words
        words = word_regex.findall(line)
        # Remove stop words from the words list
        words = [word.lower() for word in words if word.lower() not in stop_words]
        # Output each word
        for word in words:
            print("%s\t%s" % (word, 1))
    
    Command:
    hadoop jar hadoop -streaming -3.3.0.jar -file stop.py -input programs-word/input.txt -output programs-word/output mapper
    "python stop.py" 
    hdfs dfs -cat programs-word/output/*
    
    
    
    5) PAGERANK
    
    import networkx as nx
    G=nx.barabasi_albert_graph(60,41)
    pr=nx.pagerank(G,0.4)
    print(pr)
    
    
    
    8)A) MISSING VALUE RATIO:
    
    import pandas as pd
    import numpy as np
    train=pd.read_csv(r'Big_mart.csv')
    print('\n\nFeatures Before Missing Ratio : ')
    print('-----------------------------------')
    print(train.isnull().sum()/len(train)*100)
    print('\nSize of dataset before removing missing values:', train.shape)
    n = train.isnull().sum() 
    a=n/len(train)*100
    variables = train.columns
    variable = [ ]
    print('\n\nFeatures whose threshold >10, that has to removed are:')
    print('---------------------------------------------------')
    for i in range(0,len(train.columns)):
        if a[i]>=10:  
            variable.append(variables[i])
            print(variables[i])
            train = train.drop(variables[i], axis=1)
    print('\nfinal features after Missing Ratio are')
    print('------------------------------------------')
    print(train.isnull().sum()/len(train)*100)
    print('\nSize of dataset after removing missing values:', train.shape)
    
    
    8)B) LOW VARIANCE FILTER
    
    import pandas as pd
    import numpy as np
    train=pd.read_csv(r'Big_Mart.csv')
    print(train.dtypes)
    print('\n\nAfter removing other data types\n-----------------------------')
    train = train.select_dtypes(exclude = 'object')
    print(train.dtypes)
    print('\nVariance of Columns \n-------------------')
    print(train.var())
    var =[]
    var = train.var()
    numeric = train.columns
    variable = [ ]
    print('After removing low variance filter with threshold 15% is\n-------------------------------------------------------')
    for i in range(0,len(var)):
        if var[i]>1.5:   
           variable.append(numeric[i])
           print(numeric[i])
    
    
    
    8)C) HIGH CORRELATION FILTER
    
    import pandas as pd
    import numpy as np
    from sklearn import datasets
    iris = datasets.load_iris()
    X = iris.data
    y = iris.target
    df = pd.DataFrame(X)
    print(df.head())
    cor_matrix = df.corr().abs()
    print(cor_matrix)
    upper_tri =cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))
    print(upper_tri)
    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]
    print()
    print('The column to drop is' , to_drop)
    df1 = df.drop(df.columns[to_drop], axis=1)
    print()
    print(df1.head())
    
    
    9) PCA:
    
    import pandas as pd
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
    df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])
    from sklearn.preprocessing import StandardScaler
    variables = ['sepal length', 'sepal width', 'petal length', 'petal width']
    x = df.loc[:, variables].values
    y = df.loc[:,['target']].values
    x = StandardScaler().fit_transform(x)
    x = pd.DataFrame(x)
    print('After Standardization:')
    from sklearn.decomposition import PCA
    pca = PCA()
    x_pca = pca.fit_transform(x)
    x_pca = pd.DataFrame(x_pca)
    x_pca.head()
    print('After PCA:')
    print(x_pca.head())
    explained_variance = pca.explained_variance_ratio_
    print('Explained Variance Ratio : ')
    print(explained_variance)
    a=explained_variance
    variables = x_pca.columns
    variable = [ ]
    print('\n\nPrincipal Components Are :')
    for i in range(0,len(x_pca.columns)):
        if (a[i]>=0.2):
            variable.append(variables[i])
            print(variables[i])
        else:
            x_pca = x_pca.drop(variables[i], axis=1)
    print('After PCA :')
    x_pca.columns=['PC1', 'PC2']
    print(x_pca.head())
    
    10) Subgraphs
    
    from networkx.algorithms import community
    import networkx as  nx
    import matplotlib.pyplot as plt
    import random
    from itertools import combinations
    
    def draw(G, density =False):
        if len(G.nodes)==0:
            return
        plt.figure(random.randint(1,100))
        nx.draw_networkx(G)
        plt.show()
    
    
    G= nx.barbell_graph(6,1)
    draw(G)
    communitoes_generator=community.girvan_newman(G)
    top_level_communitoes = next(communitoes_generator)
    next_level_communitoes = next(communitoes_generator)
    subs=sorted(map(sorted,next_level_communitoes))
    
    for i in subs:
        if len(i) == 1:
            SG =nx.Graph()
            SG.add_node(i[0])
            draw(SG)
        sub_graph = nx.Graph()
        k=list(combinations(i,2))
        for j in k:
           sub_graph.add_edge(j[0],j[1])
        draw(sub_graph)
    
    2)  bench mark
    hadoop jar had.jar mrbench -numRuns 50
    
    2nd method
    hadoop jar ha(tab space) mrbench numRuns 50
    
    
    stress test
    hadoop jar had.jar nnbench -operation create_write -maps 12 -reduces 6 -blockSize 1 -bytestoWrite 0 -numberofFiles 100</pre></div>
  </body>
</html>
